{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bryce-R/ML/blob/master/cnn_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPI1Oco5jvCd"
      },
      "source": [
        "# A Convolutional Autoencoder for Anomaly Detection\n",
        "\n",
        "Anomaly detection is the task of finding anomalous data elements in a dataset. An anomaly is a data element that is an outlier with respect to the rest of the dataset.\n",
        "\n",
        "We are going to train an autoencoder on the MNIST dataset (that only contains numbers), and then we will look into anomalies within the MNIST dataset (i.e., images within MNIST that are somehow different than the rest of the dataset).\n",
        "\n",
        "Even though MNIST is a labeled dataset, we are going to disregard the labels for educational purposes and consider it as an unlabeled datasets.\n",
        "\n",
        "**This time we are going to use a CNN-based autoencoder.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viMdMmH2jvCf",
        "outputId": "6d8bfde4-210d-4a20-82cc-f235f8d77932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting opencv-python-headless==4.5.3.56\n",
            "  Downloading opencv_python_headless-4.5.3.56-cp37-cp37m-manylinux2014_x86_64.whl (37.1 MB)\n",
            "Collecting bokeh==2.1.1\n",
            "  Downloading bokeh-2.1.1.tar.gz (19.3 MB)\n",
            "Collecting torchvision==0.12.0\n",
            "  Downloading torchvision-0.12.0-cp37-cp37m-manylinux1_x86_64.whl (21.0 MB)\n",
            "Collecting tqdm==4.63.0\n",
            "  Downloading tqdm-4.63.0-py2.py3-none-any.whl (76 kB)\n",
            "Collecting ipywidgets==7.6.5\n",
            "  Downloading ipywidgets-7.6.5-py2.py3-none-any.whl (121 kB)\n",
            "Collecting livelossplot==0.5.4\n",
            "  Downloading livelossplot-0.5.4-py3-none-any.whl (22 kB)\n",
            "Collecting pytest==7.1.1\n",
            "  Downloading pytest-7.1.1-py3-none-any.whl (297 kB)\n",
            "Collecting pandas==1.3.5\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "Collecting seaborn==0.11.2\n",
            "  Downloading seaborn-0.11.2-py3-none-any.whl (292 kB)\n",
            "Collecting jupyterlab-widgets>=1.0.0; python_version >= \"3.6\"\n",
            "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
            "Collecting widgetsnbextension~=3.5.0\n",
            "  Downloading widgetsnbextension-3.5.2-py2.py3-none-any.whl (1.6 MB)\n",
            "Collecting py>=1.8.2\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.2.0-py3-none-any.whl (17 kB)\n",
            "Collecting tomli>=1.0.0\n",
            "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
            "Building wheels for collected packages: bokeh\n",
            "  Building wheel for bokeh (setup.py): started\n",
            "  Building wheel for bokeh (setup.py): finished with status 'done'\n",
            "  Created wheel for bokeh: filename=bokeh-2.1.1-py3-none-any.whl size=9257186 sha256=9c070067cc2b3ef87a52ee3317d85acea25c4c34aade29355c8095b86b303069\n",
            "  Stored in directory: /root/.cache/pip/wheels/f7/55/ff/f3d7554e69382d31cf7ad857cf518af9b923134fca7d925187\n",
            "Successfully built bokeh\n",
            "Installing collected packages: opencv-python-headless, bokeh, torchvision, tqdm, jupyterlab-widgets, widgetsnbextension, ipywidgets, livelossplot, py, pluggy, tomli, iniconfig, pytest, pandas, seaborn\n",
            "\u001b[33m  WARNING: The script bokeh is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script tqdm is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts py.test and pytest are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed bokeh-2.1.1 iniconfig-2.0.0 ipywidgets-7.6.5 jupyterlab-widgets-3.0.13 livelossplot-0.5.4 opencv-python-headless-4.5.3.56 pandas-1.3.5 pluggy-1.2.0 py-1.11.0 pytest-7.1.1 seaborn-0.11.2 tomli-2.0.1 torchvision-0.12.0 tqdm-4.63.0 widgetsnbextension-3.5.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt | grep -v \"already\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0_3InkqjvCg"
      },
      "source": [
        "> After installing the dependencies you need to restart your kernel. The following cell does that for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDy4VbjjjvCg",
        "outputId": "0d80ed7d-5073-4d81-98ea-3ecca6013b0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'status': 'ok', 'restart': True}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import IPython\n",
        "\n",
        "IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCSEEEhOjvCg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "from torch import nn\n",
        "import torchvision.models\n",
        "import torchvision.transforms as transforms\n",
        "import multiprocessing\n",
        "from tqdm import tqdm\n",
        "from helpers import get_data_loaders\n",
        "from helpers import seed_all\n",
        "from helpers import anomaly_detection_display\n",
        "import pandas as pd\n",
        "\n",
        "# Ensure repeatibility\n",
        "seed_all(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX39fkI4jvCg",
        "outputId": "28acdfbc-34e4-4e42-bb89-0ca325d59a4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using 48000 examples for training and 12000 for validation\n",
            "Using 10000 for testing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:178.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ]
        }
      ],
      "source": [
        "# This will get data loaders for the MNIST dataset for the train, validation\n",
        "# and test dataset\n",
        "data_loaders = get_data_loaders(batch_size=1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2z3CDU1fjvCh"
      },
      "source": [
        "### Visualize the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCnZ8he_jvCh",
        "outputId": "7869d39d-f31b-48e9-bf81-a1f10d899bab"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEgUlEQVR4nO2dSyh8YRjGzyCX3BYsyCUlZYEVK5eFhZRkJWLD3kJSZMPGrSxIyooNW41iwYZyKbG2wIbkWiI2lOa/mzxvOZgZ/znjeX6r83SO8331887bnPOdM75AIOAIDuKiPQHx/5BsIiSbCMkmQrKJkGwiEtx2+nw+fS+LMQKBgO+zfapsIiSbCMkmQrKJkGwiJJsIySZCsomQbCIkmwjJJkKyiZBsIiSbCMkmwvV+Nit+vx9yXV0d5ObmZsh7e3u/PaWIoMomQrKJ8Lk9EfKXliVVVlYGt+vr62FfUlIS5P7+fshpaWmQn56eIDc0NEA+OjoKeZ7homVJwnEcyaZCson4sz27vb0d8uzsbHA7KysL9oX7JOv+/j7k2trasM4XDurZwnEcyaZCsomI2culCQk49dHRUch9fX2Q4+I+/79+f3+HPDQ0BLmgoAByT0/Pt+fpJVTZREg2EZJNRMz27M7OTsj2evZPGB4ehjw1NQV5YmLC9e/Ly8shFxYWQr64uAh5bpFElU2EZBMh2UTETM+urq6GPD09Ddnnw0vC19fXkNva2oLb4+PjsM9mi73WbcfKzMyEnJKS4nq+aKHKJkKyiZBsImKmZ3d3d0POyMiA/Pr6Crm3txfy7u5ucHt7exv2NTY2Qq6oqIBcVVUF2d7/vry8hPzw8OB4EVU2EZJNhGQTETM9+ytsn3x5efn02NzcXMjr6+thjX1+fg75/v4+rPP9FqpsIiSbCMkm4s/07JycHMirq6uQHx8fg9vZ2dmwL9x14x/P7WVU2URINhGSTUTM9OzT09MfHR8fHw/ZPt8VSVZWVn7t3JFElU2EZBMRMx/jNTU1kO3SoJ9g//b5+RmyXRo8MzMDuaWlBbJdtrS4uBjy3H4TVTYRkk2EZBPh2Z49MjICuampCfJXlzjtY7gHBwefHru0tATZPq5TVlbmOnZJSYnrXLyCKpsIySZCsonwTM9OT0+H3Nra6nr829sbZPvI7vHxMeStra1vz8WOnZ+f73r8wsLCt88dTVTZREg2EZJNhGd6tr3eXFpa6nr85OQk5Lm5uYjNpaOjA3JiYqLr8V69Fm5RZRMh2URINhGe6dn2+rPl9vYW8vz8fMTGTk5OhmxfbWVZW1uL2Nj/E1U2EZJNhGQT4ZmebdeF2Xx2dgb55uYm5LHsa7ZsD7avurLc3d2FPHY0UWUTIdlESDYRnunZdl2XzXl5eZCLi4sh259MtH13YGAguG3vV9vXbNmxr66uIH/82ahYQpVNhGQTIdlEeKZnf0VRURHkk5OTkM9lv8PbV2Ha9WwbGxuQ7Xf+WEGVTYRkEyHZRHjmJ5XtmrPDw0PIqampkMN5ndXOzg7ksbExyJubmyGfO9roJ5WF4ziSTYVkE+GZnm0ZHByE3NXVBfmrZ6KXl5chf3x9ld/vD2tuXkY9WziOI9lUePZjXISGPsaF4ziSTYVkEyHZREg2EZJNhGQTIdlESDYRkk2EZBMh2URINhGSTYRkE+F6P1v8LVTZREg2EZJNhGQTIdlESDYR/wCAbA7fwgE9yQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 144x144 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(data_loaders['train'])\n",
        "images, labels = dataiter.next()\n",
        "images = images.numpy()\n",
        "\n",
        "# get one image from the batch\n",
        "img = np.squeeze(images[0])\n",
        "\n",
        "fig, sub = plt.subplots(figsize = (2,2))\n",
        "sub.imshow(img, cmap='gray')\n",
        "_ = sub.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05C8WHQ7jvCh"
      },
      "source": [
        "---\n",
        "## Convolutional Autoencoder\n",
        "\n",
        "Write your own CNN autoencoder. Use at least 2 blocks Convolution + ReLU + MaxPooling as _encoder_, and then an equivalent number of upsampling operations (either Transposed Convolutions+ReLU or Upsample+Conv+ReLU) followed by a Sigmoid activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqJcGJ16jvCh"
      },
      "outputs": [],
      "source": [
        "# define the NN architecture\n",
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "\n",
        "        ## encoder ##\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(32, 32, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        ## decoder ##\n",
        "        self.decoder = nn.Sequential(\n",
        "            # Undo the Max Pooling\n",
        "            nn.ConvTranspose2d(32, 32, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ConvTranspose2d(32, 32, 2, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 1, 3, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.auto_encoder = nn.Sequential(\n",
        "            self.encoder,\n",
        "            self.decoder\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # define feedforward behavior\n",
        "        # and scale the *output* layer with a sigmoid activation function\n",
        "\n",
        "        return self.auto_encoder(x)\n",
        "\n",
        "# initialize the NN\n",
        "model = Autoencoder()\n",
        "\n",
        "# if torch.cuda.is_available():\n",
        "#     device = torch.device(\"cuda\")\n",
        "#     print(\"Running on the GPU\")\n",
        "# else:\n",
        "#     device = torch.device(\"cpu\")\n",
        "#     print(\"Running on the CPU\")\n",
        "\n",
        "# model = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJo9xmo1jvCh"
      },
      "source": [
        "## Loss Function\n",
        "\n",
        "Set up here a loss function that makes sense for the task at hand (look at the lesson again if you don't remember what this should be):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4Co21sSjvCi"
      },
      "outputs": [],
      "source": [
        "# specify loss function\n",
        "criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TeAhzBPjvCi"
      },
      "source": [
        "## Training\n",
        "\n",
        "The training loop is similar to a normal training loop - however, this task is an unsupervised task. That means we do not need labels. The MNIST dataset does provide labels, of course, so we will just disregard them.\n",
        "\n",
        "Complete the training loop below. As usual, you need to perform the forward and the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M81GPYRajvCi"
      },
      "outputs": [],
      "source": [
        "# specify optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DIHQht5jvCi",
        "outputId": "653eb079-aac6-45bf-b303-7add84d059f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 47/47 [00:32<00:00,  1.43it/s]\n",
            "Validating: 100%|██████████| 12/12 [00:25<00:00,  2.08s/it]\n",
            "Training:   0%|          | 0/47 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 31.220589\tValid Loss: 9.581695\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 47/47 [00:32<00:00,  1.44it/s]\n",
            "Validating: 100%|██████████| 12/12 [00:25<00:00,  2.16s/it]\n",
            "Training:   0%|          | 0/47 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2 \tTraining Loss: 8.861354\tValid Loss: 6.407532\n"
          ]
        }
      ],
      "source": [
        "# number of epochs to train the model\n",
        "n_epochs = 50\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    ###################\n",
        "    # train the model #\n",
        "    ###################\n",
        "    for data in tqdm(desc=\"Training\", total=len(data_loaders['train']), iterable=data_loaders['train']):\n",
        "        # we disregard the labels. We use the Python convention of calling\n",
        "        # an unused variable \"_\"\n",
        "        images, _ = data\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            images = images.cuda()\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs.flatten(), images.flatten())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # update running training loss\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # Validation\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data in tqdm(desc=\"Validating\", total=len(data_loaders['valid']), iterable=data_loaders['valid']):\n",
        "            # _ stands in for labels, here\n",
        "            images, _ = data\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            outputs = model(images)\n",
        "            # calculate the loss\n",
        "            loss = criterion(outputs.flatten(), images.flatten())\n",
        "\n",
        "            # update running training loss\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # print avg training statistics\n",
        "    train_loss /= len(data_loaders['train'])\n",
        "    val_loss /= len(data_loaders['valid'])\n",
        "    print(\"Epoch: {} \\tTraining Loss: {:.6f}\\tValid Loss: {:.6f}\".format(epoch, train_loss, val_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9XDPMvDjvCi"
      },
      "outputs": [],
      "source": [
        "# Epoch: 50 \tTraining Loss: 1.359510\tValid Loss: 1.399772"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxSPRYwqjvCi"
      },
      "source": [
        "## Finding Anomalies\n",
        "Now that our autoencoder is trained we can use it to find anomalies. Let's consider the test set. We loop over all the batches in the test set and we record the value of the loss for each example separately. The examples with the highest reconstruction loss are our anomalies.\n",
        "\n",
        "Indeed, if the reconstruction loss is high, that means that our trained autoencoder could not reconstruct them well. Indeed, what the autoencoder learned about our dataset during training is not enough to describe these examples, which means they are different than what the encoder has seen during training, i.e., they are anomalies (or at least they are the most uncharacteristic examples).\n",
        "\n",
        "Let's have a look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w64NFkjnjvCi"
      },
      "outputs": [],
      "source": [
        "# Since this dataset is small we collect all the losses as well as\n",
        "# the image and its reconstruction in a dictionary. In case of a\n",
        "# larger dataset you might have to save on disk\n",
        "# (won't fit in memory)\n",
        "losses = {}\n",
        "\n",
        "# We need the loss by example (not by batch)\n",
        "loss_no_reduction = nn.MSELoss(reduction='none')\n",
        "\n",
        "idx = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in tqdm(desc=\"Testing\", total=len(data_loaders['test']),\n",
        "            iterable=data_loaders['test']\n",
        "        ):\n",
        "\n",
        "            images, _ = data\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                images = images.cuda()\n",
        "\n",
        "            # forward pass: compute predicted outputs by passing inputs to the model\n",
        "            outputs = model(images)\n",
        "\n",
        "            # calculate the loss\n",
        "            loss = loss_no_reduction(outputs, images)\n",
        "\n",
        "            # Accumulate results per-example\n",
        "            for i, l in enumerate(loss.mean(dim=[1, 2, 3])):\n",
        "                losses[idx + i] = {\n",
        "                    'loss': float(l.cpu().numpy()),\n",
        "                    'image': images[i].cpu().numpy(),\n",
        "                    'reconstructed': outputs[i].cpu().numpy()\n",
        "                }\n",
        "\n",
        "            idx += loss.shape[0]\n",
        "\n",
        "# Let's save our results in a pandas DataFrame\n",
        "df = pd.DataFrame(losses).T\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU1ENykhjvCi"
      },
      "source": [
        "Let's now display the histogram of the loss. The elements on the right (with the higher loss) are the most uncharacteristic examples. Feel free to look into `helpers.py` to see how these plots are made:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAIciPyxjvCi"
      },
      "outputs": [],
      "source": [
        "from helpers import anomaly_detection_display\n",
        "\n",
        "anomaly_detection_display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e6T6h1HjvCj"
      },
      "source": [
        "We got similar results as in the case of linear autoencoders, but the loss is on average much smaller (a sign that the network is much more capable of representing and reconstructing the dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K0sHtxIjvCj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}